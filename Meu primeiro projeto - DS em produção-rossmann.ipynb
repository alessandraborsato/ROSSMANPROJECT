{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contexto do Projeto\n",
    "\n",
    "Rossmann opera mais de 3.000 drogarias em 7 países europeus. Atualmente, os gerentes de loja da Rossmann têm a tarefa de prever suas vendas diárias com até seis semanas de antecedência. As vendas da loja são influenciadas por muitos fatores, incluindo promoções, competição, feriados escolares e estaduais, sazonalidade e localidade. Com milhares de gerentes individuais prevendo vendas com base em suas circunstâncias únicas, a precisão dos resultados pode ser bastante variada. Você pode baixar o conjunto de dados aqui: https://www.kaggle.com/c/rossmann-store-sales/data.\n",
    "\n",
    "Problema de Negócio\n",
    "Em primeiro lugar, precisamos entender qual é o nosso problema de negócios. Portanto, criamos um contexto para nos ajudar a construir a solução. Então, vamos seguir essas quatro etapas.\n",
    "\n",
    "Qual é o contexto?\n",
    "Em reunião com os líderes de cada departamento, o CEO da Rossmann fez a proposta de reformar todas as lojas.\n",
    "\n",
    "Qual é a causa?\n",
    "O CEO da Rossmann quer prever quanto cada loja vai vender nas próximas 6 semanas. Ele precisa saber se o orçamento será suficiente para fazer uma reforma em cada loja.\n",
    "\n",
    "Quem vai liderar o projeto?\n",
    "Precisamos de alguém que realmente saiba qual é o problema do negócio, porque ele vai liderar a solução. Portanto, ele é nosso stakeholder.\n",
    "\n",
    "Como ficará nossa solução?\n",
    "\n",
    "Qual é o formato?\n",
    "\n",
    "Granularidade (hora, dia, produto) ---> 6 semanas\n",
    "\n",
    "Tipo de problema (classificação, regressão, agrupamento, etc.) ---> Regressão\n",
    "\n",
    "Como vamos entregar? (painel, csv, Notebook) ---> Notebook\n",
    "\n",
    "Etapas do CRISP : QUESTAO DO NEGOCIO -> ENTENDIMENTO DO NEGOCIO -> COLETA DOS DADOS -> LIMPEZA DOS DADOS -> EXPLORACAO DOS DADOS -> MODELAGEM DOS DADOS -> ALGORITMOS DE MACHINE LEARNING\n",
    "\n",
    "Etapas do Projeto:\n",
    "\n",
    "\n",
    "Seguiremos todas essas etapas a seguir para a resolução do nosso projeto:\n",
    "\n",
    "\n",
    "0.0. IMPORTS\n",
    "\n",
    "1.0. DESCRIÇÃO DOS DADOS\n",
    "\n",
    "2.0. FEATURE ENGINEERING\n",
    "\n",
    "3.0. FILTRANDO OS DADOS\n",
    "\n",
    "4.0. ANÁLISE EXPLORATÓRIA DOS DADOS\n",
    "\n",
    "5.0. PREPARAÇÃO DOS DADOS\n",
    "\n",
    "6.0. SELEÇÃO DE FEATURES\n",
    "\n",
    "7.0. MACHINE LEARNING\n",
    "\n",
    "8.0. INTERPRETANDO OS RESULTADOS\n",
    "\n",
    "9.0. TRADUÇAO E INTERPRETACAO DO ERRO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:27:47.772594Z",
     "start_time": "2021-06-13T16:27:47.750734Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import datetime\n",
    "import warnings\n",
    "import inflection\n",
    "import numpy   as np\n",
    "import pandas  as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "import matplotlib.pyplot   as plt\n",
    "from matplotlib.gridspec     import GridSpec\n",
    "\n",
    "from scipy                   import stats    as ss\n",
    "from boruta                import BorutaPy\n",
    "from tabulate              import tabulate\n",
    "from matplotlib            import pyplot as plt\n",
    "from scipy.stats           import chi2_contingency\n",
    "from IPython.display       import Image\n",
    "from sklearn.metrics       import mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble      import RandomForestRegressor\n",
    "from sklearn.linear_model  import LinearRegression, Lasso\n",
    "from IPython.core.display  import HTML\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost               as xgb\n",
    "import pickle\n",
    "from flask                   import Flask\n",
    "from IPython.display         import Image\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [25, 12]\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:08:06.336633Z",
     "start_time": "2021-06-13T16:08:04.803747Z"
    }
   },
   "outputs": [],
   "source": [
    "df_sales_raw= pd.read_csv('train.csv',low_memory= False)\n",
    "df_store_raw= pd.read_csv('store.csv',low_memory= False)\n",
    "\n",
    "# Fazendo o merge dos dados para transformar os dois datasets em apenas 1 \n",
    "df_raw=pd.merge(df_sales_raw,df_store_raw,how='left', on='Store')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:08:06.653104Z",
     "start_time": "2021-06-13T16:08:06.339623Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Customers</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "      <th>StoreType</th>\n",
       "      <th>Assortment</th>\n",
       "      <th>CompetitionDistance</th>\n",
       "      <th>CompetitionOpenSinceMonth</th>\n",
       "      <th>CompetitionOpenSinceYear</th>\n",
       "      <th>Promo2</th>\n",
       "      <th>Promo2SinceWeek</th>\n",
       "      <th>Promo2SinceYear</th>\n",
       "      <th>PromoInterval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>592693</th>\n",
       "      <td>299</td>\n",
       "      <td>4</td>\n",
       "      <td>2014-01-16</td>\n",
       "      <td>4108</td>\n",
       "      <td>444</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>d</td>\n",
       "      <td>c</td>\n",
       "      <td>38630.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Store  DayOfWeek        Date  Sales  Customers  Open  Promo  \\\n",
       "592693    299          4  2014-01-16   4108        444     1      0   \n",
       "\n",
       "       StateHoliday  SchoolHoliday StoreType Assortment  CompetitionDistance  \\\n",
       "592693            0              0         d          c              38630.0   \n",
       "\n",
       "        CompetitionOpenSinceMonth  CompetitionOpenSinceYear  Promo2  \\\n",
       "592693                        9.0                    2012.0       0   \n",
       "\n",
       "        Promo2SinceWeek  Promo2SinceYear PromoInterval  \n",
       "592693              NaN              NaN           NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descrição dos dados das variáveis\n",
    "\n",
    "Store - um ID único para cada loja.\n",
    "\n",
    "DayOfWeek - Dia correspondente da semana.\n",
    "\n",
    "Date - O dia do acontecimento do dado.\n",
    "\n",
    "Sales - o volume de vendas.\n",
    "\n",
    "Customers - o número de clientes em um determinado dia.\n",
    "\n",
    "Open - um indicador para saber se a loja estava aberta: 0 = fechada, 1 = aberta\n",
    "\n",
    "StateHoliday - indica um feriado estadual. Normalmente todas as lojas, com poucas exceções, fecham nos feriados estaduais. Observe que todas as escolas fecham nos feriados e finais de semana. a = feriado, b = feriado da Páscoa, c = Natal, 0 = Nenhum\n",
    "\n",
    "SchoolHoliday - indica se (loja, data) foi afetado pelo fechamento de escolas públicas.\n",
    "\n",
    "StoreType - diferencia entre 4 modelos de loja diferentes: a, b, c, d\n",
    "\n",
    "Assortment - descreve um nível de sortimento: a = básico, b = extra, c = estendido\n",
    "\n",
    "CompetitionDistance - distância em metros até a loja concorrente mais próxima.\n",
    "\n",
    "CompetitionOpenSince [Month / Year] - fornece o mês e ano aproximados em que o concorrente mais próximo foi aberto.\n",
    "\n",
    "Promo - indica se uma loja está fazendo uma promoção naquele dia.\n",
    "\n",
    "Promo2 - Promo2 é uma promoção contínua e consecutiva para algumas lojas: 0 = a loja não está participando, 1 = a loja está participando.\n",
    "\n",
    "Promo2Since [Year / Week] - descreve o ano e a semana em que a loja começou a participar da Promo2.\n",
    "\n",
    "PromoInterval - descreve os intervalos consecutivos em que a Promo2 é iniciada, nomeando os meses em que a promoção é reiniciada. Por exemplo. \"Fev, maio, agosto, novembro\" significa que cada rodada começa em fevereiro, maio, agosto, novembro de qualquer ano para aquela loja.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 0.1 Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:08:06.670203Z",
     "start_time": "2021-06-13T16:08:06.656341Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', None)\n",
    "\n",
    "def cramer_v(x, y):\n",
    "    cm= np.asmatrix(pd.crosstab(x,y))\n",
    "    n= cm.sum()\n",
    "    r, k= cm.shape\n",
    "    \n",
    "    chi2= ss.chi2_contingency(cm)[0]\n",
    "    chi2corr= max(0, chi2 - (k - 1) * (r-1)/n - 1)\n",
    "    kcorr= k - (k - 1) ** 2 / (n - 1)\n",
    "    rcorr= r - (r - 1) ** 2 / (n - 1)\n",
    "    \n",
    "    return np.sqrt((chi2corr / n) / (min(kcorr - 1, rcorr - 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-5-81c2eb570571>, line 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-81c2eb570571>\"\u001b[0;36m, line \u001b[0;32m50\u001b[0m\n\u001b[0;31m    return np.mean( np.abs( ( y - yhat ) / y ) ) #np.abs: retira o valor de resultado negativo\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def cross_validation( x_training, kfold, model_name, model, verbose=False ):\n",
    "    mae_list = []\n",
    "    mape_list = []\n",
    "    rmse_list = []\n",
    "    for k in reversed( range( 1, kfold+1 ) ):\n",
    "        if verbose:\n",
    "            print( '\\nKFold Number: {}'.format( k ) )\n",
    "        # start and end date for validation \n",
    "        validation_start_date = x_training['date'].max() - datetime.timedelta( days=k*6*7)\n",
    "        validation_end_date = x_training['date'].max() - datetime.timedelta( days=(k-1)*6*7)\n",
    "\n",
    "        # filtering dataset\n",
    "        training = x_training[x_training['date'] < validation_start_date]\n",
    "        validation = x_training[(x_training['date'] >= validation_start_date) & (x_training['date'] <= validation_end_date)]\n",
    "\n",
    "        # training and validation dataset\n",
    "        # training\n",
    "        xtraining = training.drop( ['date', 'sales'], axis=1 ) \n",
    "        ytraining = training['sales']\n",
    "\n",
    "        # validation\n",
    "        xvalidation = validation.drop( ['date', 'sales'], axis=1 )\n",
    "        yvalidation = validation['sales']\n",
    "\n",
    "        # model\n",
    "        m = model.fit( xtraining, ytraining )\n",
    "\n",
    "        # prediction\n",
    "        yhat = m.predict( xvalidation )\n",
    "\n",
    "        # performance\n",
    "        m_result = ml_error( model_name, np.expm1( yvalidation ), np.expm1( yhat ) )\n",
    "\n",
    "        # store performance of each kfold iteration\n",
    "        mae_list.append(  m_result['MAE'] )\n",
    "        mape_list.append( m_result['MAPE'] )\n",
    "        rmse_list.append( m_result['RMSE'] )\n",
    "\n",
    "    return pd.DataFrame( {'Model Name': model_name,\n",
    "                          'MAE CV': np.round( np.mean( mae_list ), 2 ).astype( str ) + ' +/- ' + np.round( np.std( mae_list ), 2 ).astype( str ),\n",
    "                          'MAPE CV': np.round( np.mean( mape_list ), 2 ).astype( str ) + ' +/- ' + np.round( np.std( mape_list ), 2 ).astype( str ),\n",
    "                          'RMSE CV': np.round( np.mean( rmse_list ), 2 ).astype( str ) + ' +/- ' + np.round( np.std( rmse_list ), 2 ).astype( str ) }, index=[0] )\n",
    "\n",
    "\n",
    "def mean_percentage_error( y, yhat ):\n",
    "    return np.mean( ( y - yhat ) / y )\n",
    "     \n",
    "\n",
    "    def mean_absolute_percentage_error( y, yhat ):\n",
    "    return np.mean( np.abs( ( y - yhat ) / y ) ) #np.abs: retira o valor de resultado negativo\n",
    "\n",
    "# funcao para uso em machine learning   \n",
    "def ml_error( model_name, y, yhat ):\n",
    "    mae = mean_absolute_error( y, yhat )\n",
    "    mape = mean_absolute_percentage_error( y, yhat )\n",
    "    rmse = np.sqrt( mean_squared_error( y, yhat ) )\n",
    "    \n",
    "    return pd.DataFrame( { 'Model Name': model_name, \n",
    "                           'MAE': mae, \n",
    "                           'MAPE': mape,\n",
    "                           'RMSE': rmse }, index=[0] ) \n",
    "# df para armazenar os erros\n",
    "# y: valor real, yhat : predito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jupyter_settings():\n",
    "    %matplotlib inline\n",
    "    %pylab inline\n",
    "    plt.style.use( 'bmh' )\n",
    "    plt.rcParams['figure.figsize'] = [25, 12]\n",
    "    plt.rcParams['font.size'] = 24\n",
    "    display( HTML( '<style>.container { width:100% !important; }</style>') )\n",
    "    pd.options.display.max_columns = None\n",
    "    pd.options.display.max_rows = None\n",
    "    pd.set_option( 'display.expand_frame_repr', False )\n",
    "    sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 DATA DESCRIPTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Rename Columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:08:06.897597Z",
     "start_time": "2021-06-13T16:08:06.672707Z"
    }
   },
   "outputs": [],
   "source": [
    "df1=df_raw.copy()  #criando uma copia do data para seguranca dos dados originais\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:08:06.912665Z",
     "start_time": "2021-06-13T16:08:06.902037Z"
    }
   },
   "outputs": [],
   "source": [
    "df1.columns      #olhando os nomes das colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:08:06.933551Z",
     "start_time": "2021-06-13T16:08:06.919420Z"
    }
   },
   "outputs": [],
   "source": [
    "cols_old=['Store', 'DayOfWeek', 'Date', 'Sales', 'Customers', 'Open', 'Promo',\n",
    "       'StateHoliday', 'SchoolHoliday', 'StoreType', 'Assortment',\n",
    "       'CompetitionDistance', 'CompetitionOpenSinceMonth',\n",
    "       'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek',\n",
    "       'Promo2SinceYear', 'PromoInterval']\n",
    "snakecase=lambda x:inflection.underscore(x) \n",
    "cols_new=list(map(snakecase,cols_old))  #lambda: e a declaracao de uma funcao \n",
    "#snakecase: reencreve em minusculo e com _ como separador\n",
    "#funcao map: faz o mapeamento da funcao snakecase em todas as palavras da lista cols_old\n",
    "\n",
    "#Rename\n",
    "df1.columns=cols_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:08:06.952229Z",
     "start_time": "2021-06-13T16:08:06.941554Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Numbers of Rows:{}'.format(df1.shape[0])) \n",
    "print('Numbers of Columns:{}'.format(df1.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:08:06.975516Z",
     "start_time": "2021-06-13T16:08:06.959657Z"
    }
   },
   "outputs": [],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:08:07.178301Z",
     "start_time": "2021-06-13T16:08:06.982636Z"
    }
   },
   "outputs": [],
   "source": [
    "df1['date']=pd.to_datetime(df1['date'])  # ajustando a variavel de data \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Check NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:08:07.412240Z",
     "start_time": "2021-06-13T16:08:07.181321Z"
    }
   },
   "outputs": [],
   "source": [
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Fillout NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* competition_distance: Colocar uma distância muito grande para mostrar que não existe competição perto.\n",
    "* competition_open_since_month: Colocar o respectivo número do mês da coluna ['date'].\n",
    "* competition_open_since_year: Colocar o respectivo número do ano da coluna ['date'].\n",
    "* promo2_since_week: Colocar o respectivo número da semana da coluna ['date'].\n",
    "* promo2_since_year: Colocar o respectivo número do ano da coluna ['date'].\n",
    "* promo_interval: Preencher com 0 e criar uma nova coluna chamada ['is_promo'] onde, se o mês da coluna['date'] estiver *na coluna ['promo_interval'] retorna 1(possui promoção) e 0 para o restante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:08:07.919732Z",
     "start_time": "2021-06-13T16:08:07.415622Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#competition_distance (Na's faltantes:  2642 )\n",
    "\n",
    "df1['competition_distance']=df1['competition_distance'].apply(lambda x:2000000.0 if math.isnan(x) else x)\n",
    "# distancia em metros do competidor mais proximo . \n",
    "# math.isnam : encontra na coluna os valores sem dados \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:08:07.937636Z",
     "start_time": "2021-06-13T16:08:07.923168Z"
    }
   },
   "outputs": [],
   "source": [
    "df1['competition_distance'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:08:08.288139Z",
     "start_time": "2021-06-13T16:08:07.941392Z"
    }
   },
   "outputs": [],
   "source": [
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:08:08.377492Z",
     "start_time": "2021-06-13T16:08:08.292472Z"
    }
   },
   "outputs": [],
   "source": [
    "df1.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:09:55.728673Z",
     "start_time": "2021-06-13T16:08:08.380900Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#nao possui competidor proximo ou nao tem o dado\n",
    "#No entendimento assumo que : se tiver dados vazios(isnan.math) na coluna 'competition since mounth/year/week', eu vou pegar a coluna 'date' extrair o mes e substituindo o NA, aplicando(apply)em todas as colunas (axis 1)\n",
    "    \n",
    "#competition_open_since_month (Na's faltantes: 323348)\n",
    "#mes e ano que o competidor abriu \n",
    "df1['competition_open_since_month']=df1.apply(lambda x:x ['date'].month if math.isnan (x['competition_open_since_month'])else x ['competition_open_since_month'],axis=1)\n",
    "\n",
    "#competition_open_since_year   ( Na's faltantes:  323348)\n",
    "df1['competition_open_since_year']=df1.apply(lambda x:x ['date'].year if math.isnan (x['competition_open_since_year'])else x ['competition_open_since_year'],axis=1)\n",
    "\n",
    "#promo2_since_week             ( Na's faltantes:  508031)\n",
    "df1['promo2_since_week']=df1.apply(lambda x:x ['date'].week if math.isnan (x['promo2_since_week'])else x ['promo2_since_week'],axis=1)\n",
    "\n",
    "#promo2_since_year             ( Na's faltantes:  508031)\n",
    "df1['promo2_since_year']=df1.apply(lambda x:x ['date'].year if math.isnan (x['promo2_since_year'])else x ['promo2_since_year'],axis=1)\n",
    "\n",
    "#promo_interval                  508031\n",
    "#crio uma lista de meses, se a data estiver dentro da lista a promo2 foi ativa.\n",
    "\n",
    "month_map={1:'Jan',2:'Fev',3:'Mar',4:'Apr',5:'May',6:'Jun',7:'Jul',8:'Aug',9:'Sep',10:'Oct',11:'Nov',12:'Dec'}\n",
    "df1['promo_interval'].fillna(0,inplace=True) #fillna na coluna promo_interval com valor 0, #inplace=true para nao retornar nada, com modificacao direto na coluna \n",
    "\n",
    "df1['month_map']=df1['date'].dt.month.map(month_map) # substituindo os dados da coluna date para os meses da lista month map\n",
    "df1['is_promo']=df1[['promo_interval','month_map']].apply(lambda x:0 if x['promo_interval']==0 else 1if x['month_map']in x['promo_interval'].split(',')else 0, axis=1)                                      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:09:56.078015Z",
     "start_time": "2021-06-13T16:09:55.732441Z"
    }
   },
   "outputs": [],
   "source": [
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:09:56.468264Z",
     "start_time": "2021-06-13T16:09:56.082385Z"
    }
   },
   "outputs": [],
   "source": [
    "df1.sample(5).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Change types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:09:56.509563Z",
     "start_time": "2021-06-13T16:09:56.476665Z"
    }
   },
   "outputs": [],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:09:56.673452Z",
     "start_time": "2021-06-13T16:09:56.519678Z"
    }
   },
   "outputs": [],
   "source": [
    "df1['competition_open_since_year']=df1['competition_open_since_year'].astype(int)\n",
    "df1['competition_open_since_month']=df1['competition_open_since_month'].astype(int)\n",
    "df1['promo2_since_week']=df1['promo2_since_week'].astype(int)\n",
    "df1['promo2_since_year']=df1['promo2_since_year'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Descriptive Statistical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:09:57.131266Z",
     "start_time": "2021-06-13T16:09:56.677684Z"
    }
   },
   "outputs": [],
   "source": [
    "# separando as variaveis numericas e as categoricas \n",
    "num_attributes= df1.select_dtypes(include=['int64','float64'])\n",
    "cat_attributes= df1.select_dtypes(exclude=['int64','float64','datetime64[ns]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:09:57.261781Z",
     "start_time": "2021-06-13T16:09:57.141184Z"
    }
   },
   "outputs": [],
   "source": [
    "num_attributes.sample(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:09:57.385985Z",
     "start_time": "2021-06-13T16:09:57.283526Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_attributes.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T14:43:41.415148Z",
     "start_time": "2021-06-04T14:43:41.407058Z"
    }
   },
   "source": [
    "### 1.7.1 Numerical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:10:03.591987Z",
     "start_time": "2021-06-13T16:09:57.392056Z"
    }
   },
   "outputs": [],
   "source": [
    "# central tendency- mean, median\n",
    "ct1=pd.DataFrame(num_attributes.apply(np.mean)).T\n",
    "ct2=pd.DataFrame(num_attributes.apply(np.median)).T\n",
    "#Dispersion- std,min,max,range,skew,kurtosis\n",
    "d1=pd.DataFrame(num_attributes.apply(np.std)).T\n",
    "d2=pd.DataFrame(num_attributes.apply(min)).T\n",
    "d3=pd.DataFrame(num_attributes.apply(max)).T\n",
    "d4=pd.DataFrame(num_attributes.apply(lambda x:x.max()-x.min())).T\n",
    "d5=pd.DataFrame(num_attributes.apply(lambda x:x.skew())).T\n",
    "d6=pd.DataFrame(num_attributes.apply(lambda x:x.kurtosis())).T\n",
    "#concatenate\n",
    "m=pd.concat([d2,d3,d4,ct1,ct2,d1,d5,d6]).T.reset_index()\n",
    "m.columns=(['attributes','min','max','range','mean','median','std','skew','kurtosis'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:10:03.633746Z",
     "start_time": "2021-06-13T16:10:03.594845Z"
    }
   },
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:10:10.498190Z",
     "start_time": "2021-06-13T16:10:03.636785Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(df1['competition_distance']);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7.2  Categorical Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:10:10.931574Z",
     "start_time": "2021-06-13T16:10:10.501195Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_attributes.apply(lambda x:x.unique().shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:10:11.922286Z",
     "start_time": "2021-06-13T16:10:10.934473Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize= (17, 7))\n",
    "\n",
    "aux1=df1[(df1['state_holiday']!='0')& (df1['sales']>0)];\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "sns.boxplot(x='state_holiday',y='sales', data= aux1);\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "sns.boxplot(x='store_type',y='sales',data=aux1);\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.boxplot(x='assortment',y='sales',data=aux1);\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:39:09.685661Z",
     "start_time": "2021-06-04T15:39:09.678640Z"
    }
   },
   "source": [
    "### 2.1 Mapa das hipóteses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:10:11.948415Z",
     "start_time": "2021-06-13T16:10:11.927500Z"
    }
   },
   "outputs": [],
   "source": [
    "Image('MindMapHypothesis.png') # ferramenta coggle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 2.2 Criação de hipóteses "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T15:45:45.286016Z",
     "start_time": "2021-06-04T15:45:45.280507Z"
    },
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 2.2.1 Hipóteses lojas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "***1*** lojas com maior quadro de funcionarios deveriam vender mais\n",
    "\n",
    "***2*** lojas com maior estoque deveriam vender mais\n",
    "\n",
    "***3*** lojas com maior porte deveriam vender mais\n",
    "\n",
    "***4*** lojas com maior sortimento deveriam vender mais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 2.2.2 Hipóteses Produtos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "***1*** lojas que investem mais em marketing deveriam vender mais\n",
    "\n",
    "***2*** lojas que expoem mais o produto deveriam vender mais\n",
    "\n",
    "***3*** lojas que tem precos menores nos produtos deveriam vender mais\n",
    "\n",
    "***4*** lojas que possuem precos menores por mais tempo deveriam vender mais\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 2.2.3 Hipóteses Tempo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "***1*** lojas que tem mais feriados deveriam vender mais\n",
    "\n",
    "***2*** lojas que abrem nos primeiros 6 meses deveriam vender mais\n",
    "\n",
    "***3*** lojas que abrem nos finais de semana deveriam vender mais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 2.2.4 Priorizaçao das hipóteses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:10:11.962351Z",
     "start_time": "2021-06-13T16:10:11.951668Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Aquelas que já possuimos os dados "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "***1*** lojas com maior sortimento deveriam vender mais\n",
    "\n",
    "***2*** lojas com competidores mais proximos deveriam vender menos\n",
    "\n",
    "***3*** lojas com competidores a mais tempos deveriam vender mais\n",
    "\n",
    "***4*** lojas com promocoes ativas por mais tempos deveriam vender mais\n",
    "\n",
    "***5*** lojas com mais dias de promocao deveriam vender mais \n",
    "\n",
    "***6*** lojas com mais promocoes consecutivas deveriam vender mais\n",
    "\n",
    "***7*** lojas abertas durante os feriados deveriam vender mais\n",
    "\n",
    "***8*** lojas deveriam vender mais ao longo dos anos\n",
    "\n",
    "***9*** lojas deveriam vender mais no segundo semestre\n",
    "\n",
    "***10*** lojas deveriam vender mais depois do dia 10 de cada mes\n",
    "\n",
    "***11*** lojas deveriam vender menos aos finais de semana\n",
    "\n",
    "***12*** lojas deveriam vender menos durante feriados escolares\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:10:12.108118Z",
     "start_time": "2021-06-13T16:10:11.967019Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df2=df1.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Feature Engineering\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Vamos criar features para nos auxiliar ao longo do projeto.\n",
    "Criaremos colunas para o ano, mês, dia, semana do ano e semana com ano.\n",
    "Criaremos também uma colunas para o tempo que uma loja possui um concorrente em meses.\n",
    "Trocaremos o nome dos dados das colunas state_holiday e assortment para os valores descritos na descrição dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:11:29.232347Z",
     "start_time": "2021-06-13T16:10:12.110495Z"
    }
   },
   "outputs": [],
   "source": [
    "#desmembrando da varivel \"date\" com a funcao dt.year\n",
    "df2['year']=df2['date'].dt.year   \n",
    "df2['month']=df2['date'].dt.month\n",
    "df2['day']=df2['date'].dt.day\n",
    "df2['week_of_year']=df2['date'].dt.weekofyear\n",
    "df2['year_week']=df2['date'].dt.strftime('%Y-%W') \n",
    "\n",
    "#juntar as duas variaveis - competition since\n",
    "df2['competition_since']=df2.apply(lambda x:datetime.datetime(year=x['competition_open_since_year'],month=x['competition_open_since_month'],day=1),axis=1)\n",
    "df2['competition_time_month']=((df2['date']- df2['competition_since'])/30).apply(lambda x:x.days).astype(int)\n",
    "\n",
    "#promo since \n",
    "df2['promo_since']=df2['promo2_since_year'].astype(str)+'-'+ df2['promo2_since_week'].astype(str)\n",
    "df2['promo_since']=df2['promo_since'].apply(lambda x:datetime.datetime.strptime(x +'-1','%Y-%W-%w')- datetime.timedelta(days=7))\n",
    "df2['promo_time_week']=((df2['date']-df2['promo_since'])/7).apply(lambda x:x.days).astype(int)\n",
    "\n",
    "##assortment \n",
    "df2['assortment']=df2['assortment'].apply(lambda x:'basic' if x=='a' else 'extra' if x=='b' else 'extended')\n",
    "\n",
    "#\n",
    "df2['state_holiday']= df2['state_holiday'].apply(lambda x:'public_holiday' if x=='a' else 'easter_holiday' if x=='b' else 'christmas' if x=='c'else 'regular_day')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:11:29.261896Z",
     "start_time": "2021-06-13T16:11:29.235394Z"
    }
   },
   "outputs": [],
   "source": [
    "df2.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 3.0 Filtragem dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:11:29.550596Z",
     "start_time": "2021-06-13T16:11:29.265728Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df3=df2.copy()\n",
    "df3.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 3.1 Filtragem de linhas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:11:30.138676Z",
     "start_time": "2021-06-13T16:11:29.559955Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df3=df3[(df3['open']!=0)&(df3['sales']>0)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 3.2 Seleção de colunas "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Podemos filtrar as seguintes colunas:\n",
    "\n",
    "open: Pois depois da filtragem das linhas, todas possuem o mesmo valor 1.\n",
    "customers: Pois não sabemos quantos cliente irão entrar na loja no futuro.\n",
    "promo_interval: Pois serviu como auxiliar para a construção da coluna ['is_promo'].\n",
    "month_map: Pois serviu como auxiliar para a construção da coluna ['is_promo']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:11:30.338562Z",
     "start_time": "2021-06-13T16:11:30.141899Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols_drop=['customers','open','promo_interval','month_map']\n",
    "df3=df3.drop(cols_drop,axis=1)\n",
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:11:30.413543Z",
     "start_time": "2021-06-13T16:11:30.341253Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df4=df3.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Análise exploratória de dados "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Análise Univariada "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Response Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:11:36.525751Z",
     "start_time": "2021-06-13T16:11:30.416803Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize= (10, 7))\n",
    "sns.distplot(df4['sales'], kde= True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Numerical Variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:31:08.006864Z",
     "start_time": "2021-06-13T16:31:04.441784Z"
    }
   },
   "outputs": [],
   "source": [
    "num_attributes.hist(bins=30,figsize=(15,15));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-04T21:01:54.475076Z",
     "start_time": "2021-06-04T21:01:54.460238Z"
    }
   },
   "source": [
    "### 4.1.3 Categorical Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:11:41.072941Z",
     "start_time": "2021-06-13T16:11:41.056017Z"
    }
   },
   "outputs": [],
   "source": [
    "cat_attributes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:11:41.119225Z",
     "start_time": "2021-06-13T16:11:41.075856Z"
    }
   },
   "outputs": [],
   "source": [
    "df4['state_holiday'].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:11:41.530729Z",
     "start_time": "2021-06-13T16:11:41.125326Z"
    }
   },
   "outputs": [],
   "source": [
    "# state holiday\n",
    "a=df4[df4['state_holiday']!= 'regular_day']\n",
    "sns.countplot(a['state_holiday']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:11:55.688535Z",
     "start_time": "2021-06-13T16:11:41.543456Z"
    }
   },
   "outputs": [],
   "source": [
    "# state holiday + outros plots \n",
    "plt.figure(figsize= (10, 15))\n",
    "\n",
    "plt.subplot(3,2,1)\n",
    "a=df4[df4['state_holiday']!= 'regular_day']\n",
    "sns.countplot(a['state_holiday'])\n",
    "\n",
    "plt.subplot(3,2,2)\n",
    "sns.kdeplot(df4[df4['state_holiday']=='public_holiday']['sales'],label='public_holiday',shade=True)\n",
    "sns.kdeplot(df4[df4['state_holiday']=='easter_holiday']['sales'],label='easter_holiday',shade=True)\n",
    "sns.kdeplot(df4[df4['state_holiday']=='christmas']['sales'],label='christmas',shade=True)\n",
    "\n",
    "\n",
    "#store_type\n",
    "plt.subplot(3,2,3)\n",
    "sns.countplot(df4['store_type'])\n",
    "\n",
    "plt.subplot(3,2,4)\n",
    "sns.kdeplot(df4[df4['store_type']=='a']['sales'],label='a',shade=True)\n",
    "sns.kdeplot(df4[df4['store_type']=='b']['sales'],label='b',shade=True)\n",
    "sns.kdeplot(df4[df4['store_type']=='c']['sales'],label='c',shade=True)\n",
    "sns.kdeplot(df4[df4['store_type']=='d']['sales'],label='d',shade=True)\n",
    "\n",
    "# assortment\n",
    "plt.subplot(3,2,5)\n",
    "sns.countplot(df4['assortment'])\n",
    "\n",
    "plt.subplot(3,2,6)\n",
    "sns.kdeplot(df4[df4['assortment']=='extended']['sales'],label='extended',shade=True)\n",
    "sns.kdeplot(df4[df4['assortment']=='basic']['sales'],label='basic',shade=True)\n",
    "sns.kdeplot(df4[df4['assortment']=='extra']['sales'],label='extra',shade=True);\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 4.1 Análise Bivariada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "***1*** lojas com maior sortimento deveriam vender mais\n",
    "\n",
    "***2*** lojas com competidores mais proximos deveriam vender menos\n",
    "\n",
    "***3*** lojas com competidores a mais tempos deveriam vender mais\n",
    "\n",
    "***4*** lojas com promocoes ativas por mais tempos deveriam vender mais\n",
    "\n",
    "***5*** lojas com mais dias de promocao deveriam vender mais \n",
    "\n",
    "***6*** lojas com mais promocoes consecutivas deveriam vender mais\n",
    "\n",
    "***7*** lojas abertas durante os feriados deveriam vender mais\n",
    "\n",
    "***8*** lojas deveriam vender mais ao longo dos anos\n",
    "\n",
    "***9*** lojas deveriam vender mais no segundo semestre\n",
    "\n",
    "***10*** lojas deveriam vender mais depois do dia 10 de cada mes\n",
    "\n",
    "***11*** lojas deveriam vender menos aos finais de semana\n",
    "\n",
    "***12*** lojas deveriam vender menos durante feriados escolares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-07T23:06:16.741668Z",
     "start_time": "2021-06-07T23:04:29.824Z"
    }
   },
   "source": [
    "## H1. Lojas com maior sortimento deveriam vender mais \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Hipotese Falsa , com maior sortimento vendem menos. De acordo com a plotagem o objeto extra possui notória menor quantidade de venda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:11:57.016234Z",
     "start_time": "2021-06-13T16:11:55.690812Z"
    }
   },
   "outputs": [],
   "source": [
    "aux1= df4[['assortment', 'sales']].groupby(['assortment']).sum().reset_index()\n",
    "sns.barplot( x = 'assortment', y = 'sales', data = aux1);\n",
    "\n",
    "aux2 = df4[['year_week', 'assortment', 'sales']].groupby(['year_week', 'assortment']).sum().reset_index()\n",
    "aux2.pivot( index = 'year_week', columns = 'assortment', values = 'sales').plot();\n",
    "\n",
    "aux3 = aux2[aux2['assortment'] == 'extra']\n",
    "aux3.pivot( index = 'year_week', columns = 'assortment', values = 'sales').plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H2. Lojas com competidores mais próximos  vendem menos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Hipotese Falsa, Lojas que possuem competidores proximos vendem mais. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:11:57.852584Z",
     "start_time": "2021-06-13T16:11:57.020146Z"
    }
   },
   "outputs": [],
   "source": [
    "# agrupamento(bin) para melhor visualizacao de 1000 em 1000.\n",
    "\n",
    "aux1=df4[['competition_distance','sales']].groupby('competition_distance').sum().reset_index()\n",
    "plt.figure(figsize= (15, 10))\n",
    "\n",
    "\n",
    "bins=list(np.arange(0,20000,1000)) #np.arrange= cria um array(lista de mesmo tipo)\n",
    "\n",
    "aux1['competition_distance_binned']=pd.cut(aux1['competition_distance'],bins=bins) # cut= separa o valores da coluna 'comp_distance'no DataFrame df nas faixas etárias calculadas usando o valor do argumento bins no método pandas\n",
    "aux2=aux1[['competition_distance_binned','sales']].groupby('competition_distance_binned').sum().reset_index()\n",
    "\n",
    "\n",
    "sns.barplot(x='competition_distance_binned',y='sales',data=aux2);\n",
    "plt.xticks(rotation=90);\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:31:43.872834Z",
     "start_time": "2021-06-13T16:31:42.310657Z"
    }
   },
   "outputs": [],
   "source": [
    "aux1=df4[['competition_distance','sales']].groupby('competition_distance').sum().reset_index()\n",
    "plt.figure(figsize= (15, 15))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "sns.scatterplot(x='competition_distance', y='sales',data=aux1);\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "bins=list(np.arange(0,20000,1000)) #np.arrange= cria um array(lista de mesmo tipo)\n",
    "aux1['competition_distance_binned']=pd.cut(aux1['competition_distance'],bins=bins) # cut= separa o valores da coluna 'comp_distance'no DataFrame df nas faixas etárias calculadas usando o valor do argumento bins no método pandas\n",
    "aux2=aux1[['competition_distance_binned','sales']].groupby('competition_distance_binned').sum().reset_index()\n",
    "sns.barplot(x='competition_distance_binned',y='sales',data=aux2);\n",
    "plt.xticks(rotation=90);\n",
    "\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "x=sns.heatmap(aux1.corr(method='pearson'),annot=True);\n",
    "bottom,top=x.get_ylim()\n",
    "x.set_ylim(bottom+0.5,top-0.5);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H3.Lojas com competidores a mais tempo deveriam vender mais\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Hipotese Falsa, lojas com competidores a mais tempo vendem menos. A correlacao entre o tempo e vendas de pearson é muito fraca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:31:58.100152Z",
     "start_time": "2021-06-13T16:31:52.917831Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize= (15, 15))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "aux1 = df4[['competition_time_month', 'sales']].groupby('competition_time_month').sum().reset_index()\n",
    "aux2 = aux1[(aux1['competition_time_month'] < 120) & (aux1['competition_time_month'] != 0)]\n",
    "sns.barplot(x = 'competition_time_month', y = 'sales', data = aux2);\n",
    "plt.xticks( rotation = 90);\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "sns.regplot(x = 'competition_time_month', y = 'sales', data = aux2);\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "x = sns.heatmap(aux1.corr(method='pearson'), annot = True);\n",
    "bottom, top = x.get_ylim()\n",
    "x.set_ylim();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H4. Lojas com promoçoes ativas por mais tempo vendem mais\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T23:06:29.923883Z",
     "start_time": "2021-06-10T23:06:29.907849Z"
    }
   },
   "source": [
    "Hipotese Falsa, lojas com promocao a mais tempo vendem menos. A correlacao entre o tempo e vendas de pearson é muito fraca, muito proxima a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:32:00.607699Z",
     "start_time": "2021-06-13T16:31:58.627451Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize= (15, 15))\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "sns.scatterplot(x= 'competition_time_month', y= 'sales', data= df4[(df4['competition_time_month'] <200) & df4['competition_time_month'] != 0])\n",
    "titulo=('Tempo com Concorrência', 20);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:32:15.066825Z",
     "start_time": "2021-06-13T16:32:01.013362Z"
    }
   },
   "outputs": [],
   "source": [
    "aux1=df4[['promo_time_week','sales']].groupby('promo_time_week').sum().reset_index()\n",
    "plt.figure(figsize= (15, 15))\n",
    "\n",
    "grid=GridSpec(2,3)\n",
    "\n",
    "plt.subplot(grid[0,0])\n",
    "aux2=aux1[aux1['promo_time_week']>0]\n",
    "sns.barplot(x='promo_time_week',y='sales',data=aux2)\n",
    "plt.xticks(rotation=90);\n",
    "\n",
    "\n",
    "plt.subplot(grid[0,1])\n",
    "sns.regplot(x='promo_time_week',y='sales',data=aux2)\n",
    "\n",
    "plt.subplot(grid[1,0])\n",
    "aux3=aux1[aux1['promo_time_week']<0]\n",
    "sns.barplot(x='promo_time_week',y='sales',data=aux3)\n",
    "plt.xticks(rotation=90);\n",
    "\n",
    "plt.subplot(grid[1,1])\n",
    "sns.regplot(x='promo_time_week',y='sales',data=aux3);\n",
    "\n",
    "plt.subplot(grid[:,2])\n",
    "sns.heatmap(aux1.corr(method='pearson'),annot=True);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  H5. Lojas com mais promoçoes consecutivas vendem mais\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " Hipotese Falsa, os graficos demonstram que período do periodo extendido é irrelevante nas vendas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:32:15.642837Z",
     "start_time": "2021-06-13T16:32:15.443864Z"
    }
   },
   "outputs": [],
   "source": [
    "df4[['promo','promo2','sales']].groupby(['promo','promo2']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:32:17.908592Z",
     "start_time": "2021-06-13T16:32:15.917014Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize= (15, 7))\n",
    "\n",
    "aux1= df4[(df4['promo'] == 1) & (df4['promo2'] == 1)][['sales', 'year_week']].groupby('year_week').sum().reset_index()\n",
    "sns.lineplot(x= 'year_week', y= 'sales', data= aux1, label= 'promo e promo2', estimator= sum)\n",
    "\n",
    "aux2= df4[(df4['promo'] == 1) & (df4['promo2'] == 0)][['sales', 'year_week']].groupby('year_week').sum().reset_index()\n",
    "sns.lineplot(x= 'year_week', y= 'sales', data= aux2, label= 'promo', estimator= sum)\n",
    "plt.xticks(rotation= 90);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:32:18.964712Z",
     "start_time": "2021-06-13T16:32:18.530943Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lojas com promoções menos consecutivas vendem mais\n",
    "\n",
    "aux1 = df4[(df4['promo'] == 1) & (df4['promo2'] == 1)][['year_week', 'sales']].groupby('year_week').sum().reset_index()\n",
    "ax = aux1.plot()\n",
    "\n",
    "aux2 = df4[(df4['promo'] == 1) & (df4['promo2'] == 0)][['year_week', 'sales']].groupby('year_week').sum().reset_index()\n",
    "aux2.plot(ax=ax)\n",
    "\n",
    "ax.legend(labels = ['Tradicional & Extendida', 'Extendida']);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H6. Lojas que ficam abertas durante o feriado de Natal  vendem mais. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T23:10:53.376393Z",
     "start_time": "2021-06-10T23:10:53.365995Z"
    }
   },
   "source": [
    "Hipotese Falsa,o gráfico de barras demosntra potencial de venda relevante nos feriados públicos.Apesar de que a quantidade de dias no feriado de natal é inferior a quantidade de feriados nacionais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:32:19.880859Z",
     "start_time": "2021-06-13T16:32:19.333921Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize= (15, 7))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "aux=df4[df4['state_holiday']!='regular_day']\n",
    "aux1=aux[['state_holiday','sales']].groupby('state_holiday').sum().reset_index()\n",
    "sns.barplot(x='state_holiday',y='sales',data=aux1);\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "aux2=aux[['year', 'state_holiday','sales']].groupby(['year','state_holiday']).sum().reset_index()\n",
    "sns.barplot(x='year',y='sales',hue='state_holiday',data=aux2);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H7. Lojas devem vender mais ao longo dos anos."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Hipotese Falsa, quanto mais o tempo(em anos) passa as vendem caem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:32:21.125086Z",
     "start_time": "2021-06-13T16:32:20.251599Z"
    }
   },
   "outputs": [],
   "source": [
    "aux1= df4[['sales', 'year']].groupby('year').sum().reset_index()\n",
    "plt.figure(figsize= (15, 7))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "sns.barplot(x='year',y='sales', data=aux1);\n",
    "plt.subplot(1,3,2)\n",
    "sns.regplot(x='year',y='sales', data=aux1);\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H8. Lojas devem vender mais no segundo semestre do ano\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Hipotese Falsa,lojas vendem menos no segundo semestre.Correlacao negatica -0,72."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:32:22.595304Z",
     "start_time": "2021-06-13T16:32:21.525215Z"
    }
   },
   "outputs": [],
   "source": [
    "aux1= df4[['sales', 'month']].groupby('month').sum().reset_index()\n",
    "plt.figure(figsize= (15, 7))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "sns.barplot(x='month',y='sales', data=aux1);\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "sns.regplot(x='month',y='sales', data=aux1);\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H9. Lojas vendem mais depois do dia 10 de cada mês \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Hipotese verdadeira, na correlaçao de dias em vendas é notoria a concentracao de vendas em - 0,35."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:32:24.882141Z",
     "start_time": "2021-06-13T16:32:22.894250Z"
    }
   },
   "outputs": [],
   "source": [
    "aux1= df4[['sales', 'day']].groupby('day').sum().reset_index()\n",
    "plt.figure(figsize= (15, 7))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "sns.barplot(x='day',y='sales', data=aux1);\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "sns.regplot(x='day',y='sales', data=aux1);\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True);\n",
    "\n",
    "aux1['before_after']= aux1['day'].apply(lambda x: 'before_10_days' if x<=10 else 'after_10_days')\n",
    "aux2=aux1[['before_after','sales']].groupby ('before_after').sum().reset_index()\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "sns.barplot(x='before_after',y='sales', data=aux2);\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:32:25.743767Z",
     "start_time": "2021-06-13T16:32:25.159293Z"
    }
   },
   "outputs": [],
   "source": [
    "# gastos por dia do mes\n",
    "\n",
    "aux1= df4[['sales', 'day']].groupby('day').sum().reset_index()\n",
    "plt.figure(figsize= (15, 7))\n",
    "sns.lineplot(x= 'day', y= 'sales', data= aux1, ci= None, estimator= sum)\n",
    "plt.xticks(range(1,32));\n",
    "plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H10. Lojas deveriam vender menos no final de semana"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Hipotese verdadeira, a correçao negativa de -0,76 demonstra o resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:32:27.357410Z",
     "start_time": "2021-06-13T16:32:26.363996Z"
    }
   },
   "outputs": [],
   "source": [
    "aux1= df4[['sales', 'day_of_week']].groupby('day_of_week').sum().reset_index()\n",
    "plt.figure(figsize= (15, 7))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "sns.barplot(x='day_of_week',y='sales', data= aux1);\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "sns.regplot(x='day_of_week',y='sales', data= aux1);\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "sns.heatmap(aux1.corr(method='pearson'), annot=True);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H11. Lojas devem vender menos nos feriados escolares.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Hipotese Verdadeira, os graficos demonstram o resultados das vendas no periodo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:32:29.012529Z",
     "start_time": "2021-06-13T16:32:28.309306Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize= (15, 7))\n",
    "\n",
    "aux1= df4[['sales','school_holiday']].groupby('school_holiday').sum().reset_index()\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "sns.barplot(x='school_holiday', y='sales', data=aux1);\n",
    "\n",
    "aux2= df4[['month','school_holiday','sales']].groupby(['month','school_holiday']).sum().reset_index()\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "sns.barplot(x='month',y='sales',hue='school_holiday', data=aux2);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:32:29.398043Z",
     "start_time": "2021-06-13T16:32:29.379998Z"
    }
   },
   "outputs": [],
   "source": [
    "tab=[['Hipoteses','Conclusão','Relevância'],\n",
    "     ['H1','Falsa','baixa'],\n",
    "     ['H2','Falsa','média'],\n",
    "     ['H3','Falsa','média'],\n",
    "     ['H4','Falsa','baixa'],\n",
    "     ['H5','Falsa','baixa'],\n",
    "     ['H6','Falsa','baixa'],\n",
    "     ['H7','Falsa','média'],\n",
    "     ['H8','Falsa','alta'],\n",
    "     ['H9','Falsa','alta'],\n",
    "     ['H10','Verdadeira','alta'],\n",
    "     ['H11','Verdadeira','alta'],\n",
    "     ['H12','Verdadeira','baixa'],\n",
    "    ]\n",
    "print(tabulate(tab,headers='firstrow'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Análise Multivariada "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Encontrar as relaçoes entre as próprias variáveis. Dependência dos vetores que carregam a mesma informaçao ou  sao correlacionados e que podem ser diminuidas do dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.1. Atributos Numéricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:32:36.434464Z",
     "start_time": "2021-06-13T16:32:33.259686Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize= (15, 10))\n",
    "correlation=num_attributes.corr(method='pearson')\n",
    "sns.heatmap(correlation, annot= True, linewidths= 1);\n",
    "\n",
    "# escuros: mais relacionados , claros: menos relacionados "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.2. Atributos Categóricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:33:01.192508Z",
     "start_time": "2021-06-13T16:33:01.121120Z"
    }
   },
   "outputs": [],
   "source": [
    "df4.select_dtypes(include=\"object\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:33:02.401194Z",
     "start_time": "2021-06-13T16:33:02.397464Z"
    }
   },
   "outputs": [],
   "source": [
    "# e necessario fazer a correlacao de variaveis categoricas- Nao e possivel fazer pelo metodo de pearson.\n",
    "# E necessario usar o cramer -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:33:03.153011Z",
     "start_time": "2021-06-13T16:33:03.149233Z"
    }
   },
   "outputs": [],
   "source": [
    "#pd.crosstab(a['state_holiday'],a['store_type']) #possiveis combinacoes entre as variveis categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:33:03.753076Z",
     "start_time": "2021-06-13T16:33:03.746606Z"
    }
   },
   "outputs": [],
   "source": [
    "#ex:\n",
    "#a[(a['state_holiday']=='christmas')&(a['store_type']=='a')]    \n",
    "# o resultado 4 é a confusion matrix(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:33:04.254813Z",
     "start_time": "2021-06-13T16:33:04.231250Z"
    }
   },
   "outputs": [],
   "source": [
    "cm=pd.crosstab(a['state_holiday'],a['store_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:33:04.766343Z",
     "start_time": "2021-06-13T16:33:04.761155Z"
    }
   },
   "outputs": [],
   "source": [
    "chi2=ss.chi2_contingency(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:33:05.275832Z",
     "start_time": "2021-06-13T16:33:05.268516Z"
    }
   },
   "outputs": [],
   "source": [
    "chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:33:05.788893Z",
     "start_time": "2021-06-13T16:33:05.780899Z"
    }
   },
   "outputs": [],
   "source": [
    "#(12792.159524019908) este e o valor de chi2\n",
    "#ou\n",
    "chi2=ss.chi2_contingency(cm)[0]\n",
    "chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:33:06.089640Z",
     "start_time": "2021-06-13T16:33:06.078235Z"
    }
   },
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:33:06.916969Z",
     "start_time": "2021-06-13T16:33:06.908889Z"
    }
   },
   "outputs": [],
   "source": [
    "#n: soma de todos os valores\n",
    "n=cm.sum()\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:33:07.886402Z",
     "start_time": "2021-06-13T16:33:07.866904Z"
    }
   },
   "outputs": [],
   "source": [
    "# FORMULA v =np.sqrt((chi2/n)/(min(k-1,r-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:33:10.217851Z",
     "start_time": "2021-06-13T16:33:08.092569Z"
    }
   },
   "outputs": [],
   "source": [
    "a= df4.select_dtypes(include= 'object')\n",
    "\n",
    "a1= cramer_v(a['state_holiday'], a['state_holiday'])\n",
    "a2= cramer_v(a['state_holiday'], a['store_type'])\n",
    "a3= cramer_v(a['state_holiday'], a['assortment'])\n",
    "a4= cramer_v(a['store_type'], a['state_holiday'])\n",
    "a5= cramer_v(a['store_type'], a['store_type'])\n",
    "a6= cramer_v(a['store_type'], a['assortment'])\n",
    "a7= cramer_v(a['assortment'], a['state_holiday'])\n",
    "a8= cramer_v(a['assortment'], a['store_type'])\n",
    "a9= cramer_v(a['assortment'], a['assortment'])\n",
    "\n",
    "d= pd.DataFrame({'state_holiday': [a1, a2, a3],\n",
    "                'store_type': [a4, a5, a6],\n",
    "                'assortment': [a7, a8, a9]})\n",
    "\n",
    "d= d.set_index(d.columns)\n",
    "sns.heatmap(d, annot= True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 PREPARAÇÃO DOS DADOS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:33:11.641280Z",
     "start_time": "2021-06-13T16:33:11.407068Z"
    }
   },
   "outputs": [],
   "source": [
    "df5= df4.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Normalização\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar na seção 4.1.2, não apresentamos nenhuma variável numérica com distribuição normal, então não faremos a normalização dos dados.\n",
    "Rescala o centro para o 0 com desvio padrao igual a 1\n",
    "= variavel - media/desvio padrao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Reescaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro vamos selecionar as variáveis numéricas para depois observar as features que possuem outliers e as que não possuem para aplicar diferentes métodos de reescaling, sendo o RobustScaler para o primeiro e o MinMaxScaler para o segundo.\n",
    "Rescala para o intervalo entre 0 e 1\n",
    "Distribuicao nao Gaussianas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:33:16.648422Z",
     "start_time": "2021-06-13T16:33:16.131302Z"
    }
   },
   "outputs": [],
   "source": [
    "df5.select_dtypes(include= ['int64', 'float64']).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:33:19.880221Z",
     "start_time": "2021-06-13T16:33:18.435734Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize= (15, 7))\n",
    "\n",
    "lista= ['competition_distance', 'competition_open_since_month',\n",
    "       'competition_open_since_year', 'promo2_since_week','promo2_since_year', 'year']\n",
    "\n",
    "n= 1\n",
    "for i in lista:\n",
    "    plt.subplot(3, 3, n)\n",
    "    sns.boxplot(x= i, data= df5)\n",
    "    plt.xlabel('')\n",
    "    n+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:33:20.936245Z",
     "start_time": "2021-06-13T16:33:20.146763Z"
    }
   },
   "outputs": [],
   "source": [
    "rs= RobustScaler()  # quartis\n",
    "mms= MinMaxScaler() \n",
    "\n",
    "#competition distance\n",
    "df5['competition_distance'] = rs.fit_transform(df5[['competition_distance']].values)\n",
    "pickle.dump(rs,open('parameter/encoding_competition_distance_scalerpkl','wb'))\n",
    "\n",
    "\n",
    "#competition time month\n",
    "df5['competition_time_month'] = rs.fit_transform(df5[['competition_time_month']].values)\n",
    "pickle.dump(rs,open('parameter/encoding_competition_time_month_scaler.pkl','wb'))\n",
    "\n",
    "\n",
    "#promo time week\n",
    "df5['promo_time_week'] = mms.fit_transform(df5[['promo_time_week']].values)\n",
    "pickle.dump(rs,open('parameter/encoding_promo_time_week_scaler.pkl','wb'))\n",
    "\n",
    "\n",
    "# year\n",
    "df5['year'] = mms.fit_transform(df5[['year']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:33:33.200237Z",
     "start_time": "2021-06-13T16:33:28.358864Z"
    }
   },
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(9,8))\n",
    "sns.distplot(df5['competition_distance'],bins= 30)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Transformação variáveis categóricas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T01:05:22.318780Z",
     "start_time": "2021-06-13T01:05:22.311151Z"
    }
   },
   "source": [
    "### 5.3.1 Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENCODING = converter a variavel categorica em numerica mantendo o conteudo da variável\n",
    "\n",
    "PRINCIPAIS TIPOS: one hot encoding, label, ordinal,target,frequency,embedding\n",
    "\n",
    "LabelEncoding é uma classe de utilitário para ajudar a normalizar rótulos de forma que contenham apenas valores entre 0 e n_classes-1. Pode ser usado para transformar rótulos não numéricos em rótulos numéricos.\n",
    "\n",
    "One Hot Encoding\n",
    "Esse tipo de codificação pode ser obtido com o OneHotEncoder e get_dummies, que transforma cada recurso categórico com valores possíveis de n_categories em recursos binários de ncategories, sendo um deles 1 e todos os outros 0. Por padrão, os valores que cada recurso pode assumir são inferidos automaticamente do conjunto de dados e podem ser encontrados no atributo categorias.\n",
    "\n",
    "Ordinal Encoding\n",
    "Para converter recursos categóricos em códigos inteiros, podemos usar o OrdinalEncoder. Este estimador transforma cada recurso categórico em um novo recurso de números inteiros (0 a n_categories - 1) conforme ordenado.\n",
    "\n",
    "Vamos pegar as features que são objects e transforma-las em numéricas através dos métodos abaixo:\n",
    "\n",
    "state_holiday: OneHot Encoding\n",
    "store_type: Label Enconding\n",
    "assortment: Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:33:34.529481Z",
     "start_time": "2021-06-13T16:33:34.494083Z"
    }
   },
   "outputs": [],
   "source": [
    "df5['assortment'].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:33:37.358337Z",
     "start_time": "2021-06-13T16:33:37.331070Z"
    }
   },
   "outputs": [],
   "source": [
    "df5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:33:44.263003Z",
     "start_time": "2021-06-13T16:33:43.712358Z"
    }
   },
   "outputs": [],
   "source": [
    "# state_holiday - One Hot Encoding\n",
    "df5 = pd.get_dummies(df5, prefix=['state_holiday'], columns = ['state_holiday'])\n",
    "\n",
    "\n",
    "# store_type - Label Encoding\n",
    "le = LabelEncoder()\n",
    "df5['store_type'] = le.fit_transform(df5['store_type'])\n",
    "pickle.dump(le,open ('parameter/store_type_scaler.pkl','wb'))\n",
    "\n",
    "# assortment - Ordinal\n",
    "assortment_dict = {'basic':1,\n",
    "                  'extra': 2,\n",
    "                  'extended':3}\n",
    "\n",
    "df5['assortment'] = df5['assortment'].map(assortment_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:33:46.220352Z",
     "start_time": "2021-06-13T16:33:46.176026Z"
    }
   },
   "outputs": [],
   "source": [
    "# Após transformações\n",
    "df5.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-06-13T16:09:53.574Z"
    }
   },
   "source": [
    "### 5.3.2 Response Variable Transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objetivo de Todas as transformações: Tenta trazer distribuições que apresentam um skew mais a direita ou esquerda, mais próximo de uma normal.\n",
    "\n",
    "logarithm Transformation -Aplicar o Log em todas as variáveis resposta.\n",
    "\n",
    "Box-Cox Transformation - (Formula)\n",
    "\n",
    "Cube-Root Transformation - Extrair a raiz cúbica de todos os valores.\n",
    "\n",
    "Square-Root Transformation - Extrair a raiz quadrada de todos os valores.\n",
    "\n",
    "Sine and Cosine Transformation - ... seno e cosseno.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3 Nature Transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:34:53.677200Z",
     "start_time": "2021-06-13T16:34:41.025285Z"
    }
   },
   "outputs": [],
   "source": [
    "# nao se aplica no year pois nao se pode voltar no tempo, nao e ciclo é uma constante.\n",
    "\n",
    "# month\n",
    "df5['month_sin'] = df5['month'].apply(lambda x: np.sin(x * (2.*np.pi/12)))\n",
    "df5['month_cos'] = df5['month'].apply(lambda x: np.cos(x * (2.*np.pi/12)))\n",
    "\n",
    "# day\n",
    "df5['day_sin'] = df5['day'].apply(lambda x: np.sin(x * (2.*np.pi/30)))\n",
    "df5['day_cos'] = df5['day'].apply(lambda x: np.cos(x * (2.*np.pi/30)))\n",
    "\n",
    "# week of year\n",
    "df5['week_of_year_sin'] = df5['week_of_year'].apply(lambda x: np.sin(x * (2.*np.pi/52)))\n",
    "df5['week_of_year_cos'] = df5['week_of_year'].apply(lambda x: np.cos(x * (2.*np.pi/52)))\n",
    "\n",
    "# day of week\n",
    "df5['day_of_week_sin'] = df5['day_of_week'].apply(lambda x: np.sin(x * (2.*np.pi/7)))\n",
    "df5['day_of_week_cos'] = df5['day_of_week'].apply(lambda x: np.cos(x * (2.*np.pi/7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3.1 Logarithm Transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuição das vendas antes da transformação\n",
    "plt.subplot(1,2,1)\n",
    "sns.distplot(df5['sales'])\n",
    "\n",
    "# aplicando a transformação Log\n",
    "plt.subplot(1,2,2)\n",
    "df5['sales_log'] = np.log1p(df5['sales'])\n",
    "sns.distplot(df5['sales_log'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3.2 Box-Cox Transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuição das vendas antes da transformação\n",
    "plt.subplot(1,2,1)\n",
    "sns.distplot(df5['sales'])\n",
    "\n",
    "# aplicando a transformação Box Cox\n",
    "plt.subplot(1,2,2)\n",
    "df5['sales_box'] = ss.boxcox(df5['sales'])[0]\n",
    "sns.distplot(df5['sales_box'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3.3 Square-Root Transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuição das vendas antes da transformação\n",
    "plt.subplot(1,2,1)\n",
    "sns.distplot(df5['sales'])\n",
    "\n",
    "# aplicando a transformação Square-Root\n",
    "plt.subplot(1,2,2)\n",
    "df5['sales_sqrt'] = np.sqrt(df5['sales'])\n",
    "sns.distplot(df5['sales_sqrt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare\n",
    "fig=plt.figure(figsize=(9,8))\n",
    "\n",
    "sns.distplot(df5['sales_log'],label = 'log')\n",
    "sns.distplot(df5['sales_sqrt'], label= 'sqrt')\n",
    "sns.distplot(df5['sales_box'], label= 'box')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usaremos\n",
    "df5['sales'] = np.log1p(df5['sales'])\n",
    "\n",
    "df5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0. FEATURE SELECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:34:54.848196Z",
     "start_time": "2021-06-13T16:34:54.039417Z"
    }
   },
   "outputs": [],
   "source": [
    "df6 = df5.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occam's Razor - A explicação mais simples sobre um fenômeno observado, deveria prevalecer sobre explicações mais complexas (Princípio da Navalha de Occam)\n",
    "\n",
    "De modo geral, quanto mais simples for seu modelo, mais generalista ele será, quando mais informações (colunas), mais você especifica seu modelo para problemas específicos.\n",
    "\n",
    "Tente sempre remover as variáveis colineares, que são variáveis que explicam a mesma coisa, pois lembre-se do princípio da navalha de Occam, quanto menos variáveis, mais simples, logo remova as v. colineares sempre.\n",
    "Os 3 métodos de Seleção de Variáveis:\n",
    "    \n",
    "Seleção Univariada (Filter Methods)\n",
    "vantagens:Método Simples, Método Rápido,Fácil Entendimento\n",
    "desvantagens:Não considera a influência entre as variáveis (você corre o risco de excluir variáveis importantes quando combinadas com outras).\n",
    "Seleção por Importância / Método Embutido (Embedded Methods)\n",
    "\n",
    "Random Forest (classificação ou Regressão), Lasso Regression, Ridge Regression:\n",
    "    \n",
    "Seleção por Subset ( Wrapper Methods)\n",
    "Ele testa as variáveis uma a uma e combinada, se aumentar a accuracy, ele deixa, se permanecer a mesma ou diminuir ele exclui. SELECIONE UMA VARIVAEL-> TREINE UM MODELO DE ML -> CALCULE A PERFORMANCE DO MODELO -> A PERFORMANCE AUMENTOU? MANTEM A VARIÁVEL ->ADICIONA UMA VARIVAEL ALEATORIA -> A VARIAVEL AUMENTO? SE NAO , REMOVE-SE A VARIAVEL -> ADICIONA OUTRA VARIAVEL E FAZ TODO O CICLO NOVAMENTE ATE A ACURACIA ESTAR SATISFATORIA .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Split Dataframe Into Training and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:34:55.318464Z",
     "start_time": "2021-06-13T16:34:55.252269Z"
    }
   },
   "outputs": [],
   "source": [
    "df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:34:57.080947Z",
     "start_time": "2021-06-13T16:34:55.739412Z"
    }
   },
   "outputs": [],
   "source": [
    "# deletar variaveis pois ja derivamos no item coseno e seno, estao repetidas.\n",
    "# selecionar as variaveis com relacao ao tempo, de acordo com o solicitado no problema(6 semanas de previsao de faturamento)\n",
    "cols_drop = ['week_of_year', 'day', 'month', 'day_of_week', 'promo_since', 'competition_since', 'year_week' ]\n",
    "df6 = df6.drop( cols_drop, axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6[['store', 'date']].groupby('store').max().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agrupo por loja e venda   -\n",
    "#metodo : - datetime.timedelta(days = 6*7= - 6 semanas antes do final , 6 semanas 7 dias 6*7\n",
    "df6[['store', 'date']].groupby('store').min().reset_index()['date'][0] - datetime.timedelta(days = 6*7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:34:57.547146Z",
     "start_time": "2021-06-13T16:34:57.442471Z"
    }
   },
   "outputs": [],
   "source": [
    "df6[['store', 'date']].groupby('store').max().reset_index()['date'][0] - datetime.timedelta(days = 6*7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:34:58.102582Z",
     "start_time": "2021-06-13T16:34:57.916597Z"
    }
   },
   "outputs": [],
   "source": [
    "# selecionar a data das ultimas 6 semanas de vendas para teste\n",
    "\n",
    "# training dataset\n",
    "X_train = df6[df6['date'] < '2015-06-19']\n",
    "y_train = X_train['sales']\n",
    "\n",
    "# test dataset\n",
    "X_test = df6[df6['date'] >= '2015-06-19']\n",
    "y_test = X_test['sales']\n",
    "\n",
    "print( 'Training Min Date: {}'.format( X_train['date'].min() ) )\n",
    "print( 'Training Max Date: {}'.format( X_train['date'].max() ) )\n",
    "\n",
    "print( '\\nTest Min Date: {}'.format( X_test['date'].min() ) )\n",
    "print( 'Test Max Date: {}'.format( X_test['date'].max() ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2. Boruta as Feature Selector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:34:58.455458Z",
     "start_time": "2021-06-13T16:34:58.450227Z"
    }
   },
   "outputs": [],
   "source": [
    "#training and test dataset for Boruta\n",
    "#X_train_n = X_train.drop( ['date', 'sales'], axis=1 ).values\n",
    "#y_train_n = y_train.values.ravel() #ravel() transforma o resultado num vetor\n",
    "\n",
    "## define RandomForestRegressor\n",
    "#rf = RandomForestRegressor( n_jobs=-1 ) #padrao, roda melhor \n",
    "\n",
    "##define Boruta\n",
    "#rf= random forest\n",
    "# auto= a quantitadade de ramificacao da arvore\n",
    "# verbise e random state= padrao(pode ser modificado)\n",
    "#boruta = BorutaPy( rf, n_estimators='auto', verbose=2, random_state=42 ).fit( X_train_n, y_train_n )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1. Best Features from Boruta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:34:58.958986Z",
     "start_time": "2021-06-13T16:34:58.951379Z"
    }
   },
   "outputs": [],
   "source": [
    "#cols_selected = boruta.support_.tolist()\n",
    "\n",
    "## best features\n",
    "#X_train_fs = X_train.drop( ['date', 'sales'], axis=1 )\n",
    "#cols_selected_boruta = X_train_fs.iloc[:, cols_selected].columns.to_list()\n",
    "\n",
    "## not selected boruta\n",
    "#cols_not_selected_boruta = list( np.setdiff1d( X_train_fs.columns, cols_selected_boruta ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:34:59.305379Z",
     "start_time": "2021-06-13T16:34:59.293196Z"
    }
   },
   "outputs": [],
   "source": [
    "# guardar o resultado do boruta para nao rodar o algoritmo novamente e travar a projeto.\n",
    "\n",
    "cols_selected_boruta = [\n",
    "    'store',\n",
    "    'promo',\n",
    "    'store_type',\n",
    "    'assortment',\n",
    "    'competition_distance',\n",
    "    'competition_open_since_month',\n",
    "    'competition_open_since_year',\n",
    "    'promo2',\n",
    "    'promo2_since_week',\n",
    "    'promo2_since_year',\n",
    "    'competition_time_month',\n",
    "    'promo_time_week',\n",
    "    'day_of_week_sin',\n",
    "    'day_of_week_cos',\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'day_sin',\n",
    "    'day_cos',\n",
    "    'week_of_year_sin',\n",
    "    'week_of_year_cos']\n",
    "\n",
    "# columns to add\n",
    "feat_to_add = ['date', 'sales']\n",
    "\n",
    "cols_selected_boruta_full = cols_selected_boruta.copy()\n",
    "cols_selected_boruta_full.extend(feat_to_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.0. MACHINE LEARNING MODELS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qual e o melhor modelo de ML para este projeto? Average model,linear regression,linear regression regularized,random forest regressor ou XGBost regressor?\n",
    "\n",
    "Porque aplicar o ML? Aprender o comportamento de vendas com as varivaeis disponiveis e entao generalizar para o futuro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X_train[cols_selected_boruta]\n",
    "x_test = X_test[cols_selected_boruta]\n",
    "\n",
    "# Time Series Data Preparation\n",
    "x_training = X_train[cols_selected_boruta_full]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Average Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:34:59.780216Z",
     "start_time": "2021-06-13T16:34:59.674915Z"
    }
   },
   "outputs": [],
   "source": [
    "aux1 = x_test.copy() # cria copia para proteger dados originais\n",
    "aux1['sales'] = y_test.copy()#\n",
    "\n",
    "# prediction\n",
    "aux2 = aux1[['store', 'sales']].groupby('store').mean().reset_index().rename(columns = {'sales': 'predictions'})\n",
    "aux1 = pd.merge(aux1, aux2, how = 'left', on = 'store') # reescreveu aux1\n",
    "yhat_baseline = aux1['predictions']  # yhat : estimado\n",
    "\n",
    "# performance\n",
    "baseline_result = ml_error('Average Model', np.expm1(y_test), np.expm1(yhat_baseline)) \n",
    "# ml error cria a funcao para todo o modelo ,#  ver 5.3.2 (log1p) - expm1 : variavel exponencial, volta ao varlor original antes do log1p\n",
    "# ver funcoes \n",
    "\n",
    "baseline_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:34:59.787557Z",
     "start_time": "2021-06-13T16:34:52.534Z"
    }
   },
   "outputs": [],
   "source": [
    "# ver funcoes \n",
    "# model\n",
    "lr = LinearRegression().fit(x_train, y_train) # .fit (): treina o modelo\n",
    "\n",
    "# prediction\n",
    "yhat_lr = lr.predict(x_test)\n",
    "\n",
    "# performance\n",
    "lr_result = ml_error('Linear Regression', np.expm1(y_test), np.expm1(yhat_lr))\n",
    "lr_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.1 Linear Regression Model - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_result_cv = cross_validation(x_training, 5 ,'Linear Regression', lr, verbose=True)\n",
    "lr_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Linear Regression Lasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:34:59.790663Z",
     "start_time": "2021-06-13T16:34:53.198Z"
    }
   },
   "outputs": [],
   "source": [
    "# model\n",
    "lrr = Lasso(alpha = 0.01).fit(x_train, y_train) # alpha: o quanto vc quer de relevancia , que seja um valor comparavel\n",
    "\n",
    "# prediction\n",
    "yhat_lrr = lrr.predict(x_test)\n",
    "\n",
    "# performance\n",
    "lrr_result = ml_error('Linear Regression - Lasso', np.expm1(y_test), np.expm1(yhat_lrr))\n",
    "lrr_result\n",
    "# modelos lineares possuem pior resultado, contem mais erros.Necessario implementar modelos nao lineares\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.1 Linear Regression - Lasso - Cross Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lasso_cv = cross_validation(x_training,5,'Lasso', lrr, verbose=False)\n",
    "Lasso_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:34:59.798763Z",
     "start_time": "2021-06-13T16:34:54.695Z"
    }
   },
   "outputs": [],
   "source": [
    "# model\n",
    "rf = RandomForestRegressor(n_estimators = 60, n_jobs = -1, random_state = 42 ).fit(x_train, y_train)\n",
    "\n",
    "# prediction\n",
    "yhat_rf = rf.predict(x_test )\n",
    "\n",
    "# performance\n",
    "rf_result = ml_error( 'Random Forest Regressor', np.expm1( y_test ), np.expm1( yhat_rf ) )\n",
    "rf_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4.1 Random Forest Regressor - Cross Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cv = cross_validation(x_training,5,'Random Forest', rf)\n",
    "rf_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 XGBoost Regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:34:59.806601Z",
     "start_time": "2021-06-13T16:34:56.516Z"
    }
   },
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "model_xgb = xgb.XGBRegressor(objective = 'reg:squarederror',\n",
    "                             n_estimators = 100, \n",
    "                             eta = 0.01,\n",
    "                             max_depth = 10,\n",
    "                             subsample = 0.7,\n",
    "                             colsample_bytee = 0.9).fit(x_train, y_train)\n",
    "\n",
    "# prediction#yhat_xgb = model_xgb.predict(x_test)\n",
    "\n",
    "# performance\n",
    "xgb_result = ml_error('XGBoost Regressor', np.expm1(y_test), np.expm1(yhat_xgb))\n",
    "xgb_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5.1 Xgboost - Cross Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xbg_cv = cross_validation(x_training,5,'Xgboost', model_xgb)\n",
    "xbg_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6. Compare Model's Performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6.1. Single Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_result = pd.concat([baseline_result, lr_result, lrr_result, rf_result, xgb_result]  )\n",
    "modelling_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6.2. Real Performance - Cross Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:34:59.814179Z",
     "start_time": "2021-06-13T16:34:58.715Z"
    }
   },
   "outputs": [],
   "source": [
    "modelling_result_cv = pd.concat([lr_result_cv, Lasso_cv, rf_cv, xbg_cv]  )\n",
    "modelling_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.0 HYPERPARAMETER FINE TUNNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM SEARCH : Define os valores para os parametros aleatoriamente (pros:mais rapido e mais facil, desvantagem: pode testar 2x os mesmos valores, pode nunca conseguir o melhor conjunto por ser aleatorio)\n",
    "\n",
    "GRID SEARCH:Define todas as combinacoes possiveis de valores que os hiperparametros podem assumir(pros: vai encontrar o melhor, contra: demora muito tempo de ate meses)\n",
    "\n",
    "BAYESIAN SEARCH: Define os valores para o s hiperparametros baseado na teoria de Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Random Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'n_estimators': [1500 ,1700 ,2500 ,2300 ,3500],  # original estimators \n",
    "        'eta': [0.01 , 0.03],\n",
    "        'max_depth': [3, 5, 9],\n",
    "        'subsample': [0.1, 0.5, 0.7],\n",
    "        'colsample_bytree':[0.3, 0.7, 0.9],\n",
    "        'min_child_weight':[3 ,8 ,15]\n",
    "        }\n",
    "MAX_EVAL = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_result = pd.DataFrame()\n",
    "\n",
    "for i in range(MAX_EVAL):\n",
    "    Chosse values for parameter ramndomly\n",
    "    hp = {k: random.sample(v,1)[0] for k,v in param.items() }\n",
    "    print(hp)\n",
    "    model\n",
    "    model_xgb = xgb.XGBRegressor(objective = 'reg:squarederror',\n",
    "                                  n_estimatiors =hp['n_estimators'],\n",
    "                                 eta = hp['eta'],\n",
    "                                 max_depth=hp['max_depth'],\n",
    "                                 subsample =hp['subsample'],\n",
    "                                 colsample_bytee = hp['colsample_bytree'],\n",
    "                                 min_child_weight = hp['min_child_weight'])\n",
    "\n",
    "    \n",
    "    #performance\n",
    "    result = cross_validation(x_training, 5 ,'XGBoost Regressor ', model_xgb, verbose = False )\n",
    "    final_result = pd.concat([final_result, result])\n",
    "\n",
    "final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Final Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param_tuned = {'n_estimators':3000 ,\n",
    "       'eta': 0.03,\n",
    "       'max_depth':5,\n",
    "       'subsample':0.7,\n",
    "       'colsample_bytree':0.7,\n",
    "       'min_child_weight':3\n",
    "       }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #model final\n",
    "model_xgb_tuned = xgb.XGBRegressor(objective = 'reg:squarederror',\n",
    "                                 n_estimatiors =param_tuned['n_estimators'],\n",
    "                                 eta = param_tuned['eta'],\n",
    "                                 max_depth=param_tuned['max_depth'],\n",
    "                                 subsample =param_tuned['subsample'],\n",
    "                                 colsample_bytee = param_tuned['colsample_bytree'],\n",
    "                                 min_child_weight = param_tuned['min_child_weight']).fit(x_train, y_train)\n",
    "\n",
    "#predciton\n",
    "yhat_xgb_tuned = model_xgb_tuned.predict(x_test)\n",
    "\n",
    "#performance\n",
    "xgb_result_tuned = ml_error('XGboost Regressor', np.expm1(y_test), np.expm1(yhat_xgb_tuned))\n",
    "xgb_result_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "rf = RandomForestRegressor(n_estimators=300, n_jobs=-1,random_state=42).fit(x_train, y_train)\n",
    "\n",
    "#prediction\n",
    "yhat_rf = rf.predict(x_test)\n",
    "\n",
    "#performance\n",
    "rf_result = ml_error('Random Forest Regressor', np.expm1(y_test), np.expm1(yhat_rf) )\n",
    "rf_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.0. Tradução e interpretação do erro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9 = X_test[cols_selected_boruta_full]\n",
    "\n",
    "#Rescale\n",
    "df9['sales'] = np.expm1(df9['sales'])\n",
    "df9['predictions'] = np.expm1( yhat_rf )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Business Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sum prediction\n",
    "df91 = df9[['store', 'predictions']].groupby('store').sum().reset_index()\n",
    "\n",
    "# MAE and MAPE\n",
    "df9_aux1 = df9[['store','sales','predictions']].groupby('store').apply(lambda x: mean_absolute_error(x['sales'],x['predictions'])).reset_index().rename(columns = {0:'MAE'})\n",
    "df9_aux2 = df9[['store','sales','predictions']].groupby('store').apply(lambda x: mean_absolute_percentage_error(x['sales'],x['predictions'])).reset_index().rename(columns = {0:'MAPE'})\n",
    "\n",
    "#Merge\n",
    "df9_aux3 = pd.merge(df9_aux1, df9_aux2, how='inner', on='store')\n",
    "df92= pd.merge(df91, df9_aux3, how='inner', on='store')\n",
    "\n",
    "#Scenarios\n",
    "df92['worst_scenario'] = df92['predictions'] - df92['MAE']\n",
    "df92['best_scenario'] = df92['predictions'] + df92['MAE']\n",
    "\n",
    "#Order columns\n",
    "df92 = df92[['store','predictions', 'worst_scenario','best_scenario','MAE','MAPE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df92.sort_values('MAE', ascending = False).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df92.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='store', y='MAPE', data=df92)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Total Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df93 =df92[['predictions','worst_scenario','best_scenario']].apply(lambda x: np.sum(x), axis=0).reset_index().rename(columns={'index':'scenarios',0:'values'})\n",
    "df93['values'] = df93['values'].map('R${:,.2f}'.format)\n",
    "df93"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 Machine Learning Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9['error'] = df9['sales'] - df9['predictions']\n",
    "df9['error_rate'] = df9['predictions'] / df9['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2,2,1)\n",
    "sns.lineplot(x='date', y='sales', data=df9, label='SALES')\n",
    "sns.lineplot(x='date', y='predictions', data=df9, label='PREDICITONS')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "sns.lineplot(x='date', y='error_rate', data= df9)\n",
    "plt.axhline(1, linestyle = '--')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "sns.distplot(df9['error'])\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "sns.scatterplot(df9['predictions'], df9['error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(xgb_result_tuned, open ('parameters/model_xgb_rossman.pkl', 'wb'))\n",
    "pickle.dump(rf_result, open ('parameters/model_rf_rossman.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
